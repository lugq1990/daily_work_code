{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here should add a logic that we could do some other process logic like: <, >, ==, != etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compare_main(df1, df2, cols=None, compare_op='=='):\n",
    "    \"\"\"Main compare logic for given columns.\n",
    "    \n",
    "    Support with <, >, !=, == etc. could be used for like others.\n",
    "    \"\"\"\n",
    "    if not cols:\n",
    "        # if `cols` is not provided, then we would try to get full columns\n",
    "        cols = df1.columns\n",
    "    \n",
    "    if compare_op == '==':\n",
    "        return compare_dfs_shape(df1, df2, cols=cols)\n",
    "    elif compare_op != '==':\n",
    "        # we could try to compare other operators logic here\n",
    "        for col in cols:\n",
    "            # DF should be checked first to satisfied dtype \n",
    "            tmp_df1 = df1[col]\n",
    "            tmp_df2 = df2[col]\n",
    "            # we could only support with `float64`, `int64` could be used for `<` etc.\n",
    "            supported_dtypes = [np.float64, np.int64]\n",
    "            tmp_df1_dtype = tmp_df1.dtypes.type\n",
    "            tmp_df2_dtype = tmp_df2.dtypes.type\n",
    "            \n",
    "            if tmp_df1_dtype not in supported_dtypes or tmp_df2_dtype not in supported_dtypes:\n",
    "                print(\"For {} operator, only float and int type is supported!\".format(compare_op))\n",
    "            \n",
    "            if compare_op == \"<\":\n",
    "                return (tmp_df1 < tmp_df2).all()\n",
    "            elif compare_op == '>':\n",
    "                return (tmp_df1 > tmp_df2).all()\n",
    "            elif compare_op == '!=':\n",
    "                return (tmp_df1 != tmp_df2).all()\n",
    "    else:\n",
    "        raise ValueError(\"Not supported operator: {}\".format(compare_op))\n",
    "\n",
    "    \n",
    "\n",
    "def compare_dfs_shape(df1, df2, cols=None):\n",
    "    \"\"\"Compare two DFs is same or not?\n",
    "    \n",
    "    We need to handle NAN and duplicate records!\n",
    "    \"\"\"\n",
    "    if not cols:\n",
    "        cols = df1.columns\n",
    "    \n",
    "    # should based on each column\n",
    "    is_same = True\n",
    "    for col in cols:\n",
    "        tmp_1 = df1[col].dropna()\n",
    "        tmp_2 = df2[col].dropna()\n",
    "        if len(tmp_1) != len(tmp_2):\n",
    "            is_same = False\n",
    "        counter1 = Counter(tmp_1).items()\n",
    "        counter2 = Counter(tmp_2).items()\n",
    "        if counter1 != counter2:\n",
    "            is_same = False\n",
    "        \n",
    "        if not is_same:\n",
    "            return False\n",
    "        \n",
    "    return is_same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigquery side\n",
    "\n",
    "I think at least for now, **Bigquery** should be the base, and we could try to process data from DV part and try to process them like BQ type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_file = [x for x in os.listdir() if x.endswith('json') and x.lower().startswith('cloud')][0]\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Name: a, type: INTEGER\nName: b, type: FLOAT\nName: c, type: FLOAT\nName: d, type: STRING\nName: e, type: FLOAT\nName: f, type: INTEGER\nName: g, type: DATETIME\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    a    b        c     d     e      f          g\n",
       "0   2  1.1  0.45930  TEST  12.3  12999 2020-02-20\n",
       "1  23  NaN  0.14593  None  13.3  23199 2020-02-21"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th>d</th>\n      <th>e</th>\n      <th>f</th>\n      <th>g</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1.1</td>\n      <td>0.45930</td>\n      <td>TEST</td>\n      <td>12.3</td>\n      <td>12999</td>\n      <td>2020-02-20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23</td>\n      <td>NaN</td>\n      <td>0.14593</td>\n      <td>None</td>\n      <td>13.3</td>\n      <td>23199</td>\n      <td>2020-02-21</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dataset_name = \"auto_test\"\n",
    "table_name = \"sample_data\"\n",
    "\n",
    "# dataset = client.get_dataset(dataset_name)\n",
    "# table = dataset.table(table_name)\n",
    "\n",
    "# make a easy SQL to get data from Bigquery\n",
    "sample_sql = \"select * from {}.{}\".format(dataset_name, table_name)\n",
    "\n",
    "query = client.query(sample_sql)\n",
    "result = query.result()\n",
    "\n",
    "table_schema_list = [(s.name, s.field_type) for s in result.schema]\n",
    "\n",
    "for n, t in table_schema_list:\n",
    "    print(\"Name: {}, type: {}\".format(n, t))\n",
    "\n",
    "df_bq = query.to_dataframe()\n",
    "\n",
    "df_bq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load local JSON file like stream JSON string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('sample.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>45.93%</td>\n",
       "      <td>TEST</td>\n",
       "      <td>12.3</td>\n",
       "      <td>12,999</td>\n",
       "      <td>2020/02/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.59%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.3</td>\n",
       "      <td>23,199</td>\n",
       "      <td>2020/02/21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b       c     d     e       f           g\n",
       "0   2  1.1  45.93%  TEST  12.3  12,999  2020/02/20\n",
       "1  23  NaN  14.59%   NaN  13.3  23,199  2020/02/21"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make column `c f g` to string for converting\n",
    "df['c'] = df['c'].map(lambda x: str(x*100) + \"%\")\n",
    "df['f'] = df['f'].map(lambda x: str(x)[:2] + ',' + str(x)[2:])\n",
    "df['g'] = df['g'].map(lambda x: '/'.join(str(x.strftime('%Y-%m-%d')).split('-')))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write back into server and read it from local as a stream\n",
    "json_df = df.to_json()\n",
    "\n",
    "json_file_name = \"sample.json\"\n",
    "with open(json_file_name, 'w') as f:\n",
    "    json.dump(json_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from json to make pandas to infer it\n",
    "with open(json_file_name, 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>45.93%</td>\n",
       "      <td>TEST</td>\n",
       "      <td>12.3</td>\n",
       "      <td>12,999</td>\n",
       "      <td>2020/02/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.59%</td>\n",
       "      <td>None</td>\n",
       "      <td>13.3</td>\n",
       "      <td>23,199</td>\n",
       "      <td>2020/02/21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b       c     d     e       f           g\n",
       "0   2  1.1  45.93%  TEST  12.3  12,999  2020/02/20\n",
       "1  23  NaN  14.59%  None  13.3  23,199  2020/02/21"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json = pd.read_json(json_data)\n",
    "\n",
    "df_json.columns = df_bq.columns\n",
    "\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a      int64\n",
       "b    float64\n",
       "c     object\n",
       "d     object\n",
       "e    float64\n",
       "f     object\n",
       "g     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a             int64\n",
       "b           float64\n",
       "c           float64\n",
       "d            object\n",
       "e           float64\n",
       "f             int64\n",
       "g    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bq.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could find that if origin data is basic data structure, then for **NULL** will be ignored and pandas will infer correctly, if original data is string type, then **NULL** will be converted into **NONE** based on pandas, this should be taken care.\n",
    "\n",
    "#### Noted\n",
    "Don't need to get out of **Object** type as if data in BQ is object, then it's string, then we should do compare content of each DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same data type columns: a\tb\td\te\n",
      "Diff data type columns: c\tf\tg\n"
     ]
    }
   ],
   "source": [
    "# get two dataframe's data types, and get the same data type's columns\n",
    "json_dtype = dict(df_json.dtypes)\n",
    "bq_dtype = dict(df_bq.dtypes)\n",
    "\n",
    "same_dtype_cols = []\n",
    "other_dtype_cols = []\n",
    "for k, _ in json_dtype.items():\n",
    "    if json_dtype[k] == bq_dtype[k]:\n",
    "        same_dtype_cols.append(k)\n",
    "    else:\n",
    "        other_dtype_cols.append(k)\n",
    "        \n",
    "print(\"Same data type columns: {}\".format('\\t'.join(same_dtype_cols)))\n",
    "print(\"Diff data type columns: {}\".format('\\t'.join(other_dtype_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try to compore same data type column for these 2 DFs.\n",
    "def compare_same_types(same_dtype_cols):\n",
    "    same_df_json = df_json[same_dtype_cols]\n",
    "    same_df_bq = df_bq[same_dtype_cols]\n",
    "\n",
    "    return compare_dfs_shape(same_df_json, same_df_bq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_same_types(same_dtype_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process with not same type's columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.93%</td>\n",
       "      <td>12,999</td>\n",
       "      <td>2020/02/20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.59%</td>\n",
       "      <td>23,199</td>\n",
       "      <td>2020/02/21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        c       f           g\n",
       "0  45.93%  12,999  2020/02/20\n",
       "1  14.59%  23,199  2020/02/21"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the other not same column then we need to try to process them each column directly\n",
    "df_json[other_dtype_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.45930</td>\n",
       "      <td>12999</td>\n",
       "      <td>2020-02-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.14593</td>\n",
       "      <td>23199</td>\n",
       "      <td>2020-02-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         c      f          g\n",
       "0  0.45930  12999 2020-02-20\n",
       "1  0.14593  23199 2020-02-21"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bq[other_dtype_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Datetime type from BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_date_columns(df_bq=df_bq, df_json=df_json):\n",
    "    # If both of them are date type, then we could use pandas.to_datetime try to convert them into a normal datetime, , and they will be same,\n",
    "    # otherwise we will get error then should be False returned.\n",
    "    \n",
    "    date_cols = [k for k, v in bq_dtype.items() if v.name.startswith('datetime64')]\n",
    "    date_df_bq = df_bq[date_cols]\n",
    "    date_df_json = df_json[date_cols]\n",
    "    \n",
    "    if date_df_bq.shape != date_df_json.shape:\n",
    "        return False\n",
    "    \n",
    "    sati = True\n",
    "    try:\n",
    "        # it's fine if we have many columns by using `apply`\n",
    "        date_df_bq = date_df_bq.apply(pd.to_datetime)\n",
    "    except:\n",
    "        sati = False\n",
    "    \n",
    "    try:\n",
    "        date_df_json = date_df_json.apply(pd.to_datetime)\n",
    "    except:\n",
    "        sati = False\n",
    "    \n",
    "    if not sati:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    return compare_dfs_shape(date_df_bq, date_df_json)\n",
    "\n",
    "compare_date_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Compare other types\n",
    "\n",
    "We could just try to compare others with string types will be fine, just remove some special characters, and compare them.\n",
    "We need to process each column with some pre-defined rules to compare! Like: `%`, `,`, `&`, `$`, etc.\n",
    "\n",
    "I think except for special `%`, others could just with replacement will be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c', 'f'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_cols = [k for k, v in bq_dtype.items() if v.name.startswith('datetime64')]\n",
    "other_not_sati_cols = set(list(bq_dtype.keys())) - set(date_cols) - set(same_dtype_cols)\n",
    "other_not_sati_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        c       f\n",
      "0  45.93%  12,999\n",
      "1  14.59%  23,199\n",
      "         c      f\n",
      "0  0.45930  12999\n",
      "1  0.14593  23199\n"
     ]
    }
   ],
   "source": [
    "other_sati_json_df = df_json[other_not_sati_cols]\n",
    "other_sati_bq_df = df_bq[other_not_sati_cols]\n",
    "\n",
    "print(other_sati_json_df.head())\n",
    "print(other_sati_bq_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# This should base on JSON data only, as BQ won't accept this.\n",
    "# We could try to convert full columns into `string`, and try to get\n",
    "# special: % from dataframe\n",
    "\n",
    "def compare_percen_data(other_sati_json_df=other_sati_json_df, \n",
    "                        other_sati_bq_df=other_sati_bq_df,\n",
    "                        float_round_estimation = 4,\n",
    "                        per_threshould = .9):\n",
    "    other_sati_json_df = other_sati_json_df.astype(str)\n",
    "\n",
    "    # Loop each columns to get percentage columns.\n",
    "    percen_cols = []\n",
    "    for col in other_sati_json_df.columns:\n",
    "        per_num = other_sati_json_df[col].map(lambda x: True if \"%\" in x else False).sum()\n",
    "        null_num = other_sati_json_df[col].isnull().sum()\n",
    "        if per_num:\n",
    "            if null_num:\n",
    "                if per_num / (null_num + per_num) >= per_threshould:\n",
    "                    percen_cols.append(col)\n",
    "            else:\n",
    "                if per_num / len(other_sati_json_df) >= per_threshould:\n",
    "                    percen_cols.append(col)\n",
    "\n",
    "    # If we have get percentage columns, then need to convert them into float\n",
    "    other_sati_json_df[percen_cols] = other_sati_json_df[percen_cols].applymap(lambda x: float(x.replace('%', ''))/100)\n",
    "    \n",
    "    # Key notes here: WE SHOULDN'T COMPARE FLOAT, SHOULD CONVERT INTO STRING!\n",
    "    # convert BQ df either, so could compare easy...Let's just hard-code this for 4-digits to keep\n",
    "    per_convert_json = other_sati_json_df[percen_cols].applymap(lambda x: \"%.4f\" %  round(x, float_round_estimation))\n",
    "    per_convert_bq = other_sati_bq_df[percen_cols].applymap(lambda x: \"%.4f\" % round(x, float_round_estimation))\n",
    "\n",
    "\n",
    "    return compare_dfs_shape(per_convert_json, per_convert_bq), percen_cols\n",
    "\n",
    "per_compare_res, per_cols = compare_percen_data()\n",
    "print(per_compare_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LET'S PROCESS OTHER TYPES DATA.\n",
    "# remove full special characters that we may face.\n",
    "import re\n",
    "import string\n",
    "special_characters = re.escape(string.punctuation)\n",
    "\n",
    "def compare_other_sati_columns(other_sati_bq_df=other_sati_bq_df,\n",
    "                              other_sati_json_df=other_sati_json_df):\n",
    "    other_str_cols = set(list(other_sati_bq_df.columns)) - set(per_cols)\n",
    "\n",
    "    def remove_spe_cha(x):\n",
    "        return re.sub(r\"[\" + special_characters + \"]\", \"\", str(x))\n",
    "\n",
    "    other_str_bq_df = other_sati_bq_df[other_str_cols].applymap(lambda x: remove_spe_cha(x))\n",
    "    other_str_json_df = other_sati_json_df[other_str_cols].applymap(lambda x: remove_spe_cha(x))\n",
    "\n",
    "    return compare_dfs_shape(other_str_bq_df, other_str_json_df)\n",
    "    \n",
    "    \n",
    "compare_other_sati_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python365jvsc74a57bd0a8894775e8aeee242aa6e141c386e0510b6f2108a9b3d20c3f118f8cb925642c",
   "display_name": "Python 3.6.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}