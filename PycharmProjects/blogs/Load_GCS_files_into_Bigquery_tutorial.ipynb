{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naiaecrYDNpR"
   },
   "outputs": [],
   "source": [
    "! pip install google-cloud-bigquery --quiet"
   ]
  },
  {
   "source": [
    "# with SA\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "\n",
    "sa_file_name = [x for x in os.listdir('.') if x.endswith('.json')][0]\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = sa_file_name"
   ],
   "cell_type": "code",
   "metadata": {
    "id": "nLSb1GUmDVf8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVcQtBeMFGgB"
   },
   "outputs": [],
   "source": [
    "table_str = \"\"\"sessionid int64,\n",
    "    sessionseqnum int64,\n",
    "    nodeid int64,\n",
    "    profileid int64,\n",
    "    resourceid int64,\n",
    "    startdatetime timestamp,\n",
    "    enddatetime timestamp,\n",
    "    qindex int64,\n",
    "    gmtoffset int64,\n",
    "    ringtime int64,\n",
    "    talktime int64,\n",
    "    holdtime int64,\n",
    "    worktime int64,\n",
    "    callwrapupdata string,\n",
    "    callresult int64,\n",
    "    dialinglistid int64,\n",
    "    extractdatetime timestamp \"\"\"\n",
    "\n",
    "columns_list = [x.replace('\\n', '').strip().replace('int64', 'integer').split(' ') for x in table_str.split(',')]\n",
    "\n",
    "schema_list = []\n",
    "for x in columns_list:\n",
    "  schema_list.append(bigquery.SchemaField(x[0], x[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2uQodLFF_hU",
    "outputId": "9bcaba7b-0410-4d7b-ce1f-925dc63bef12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.LoadJob at 0x7fc5da01f5c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "table_name = \"uccx_22042_brazil.agent_connection_detail_dump\"\n",
    "file_uri = \"gs://gcs_file_to_bq_new/AgentConnectionDetail_brazil_test.csv\"\n",
    "\n",
    "\n",
    "# config job config\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=schema_list,\n",
    "    skip_leading_rows=1,\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    field_delimiter='|'\n",
    ")\n",
    "\n",
    "load_job = client.load_table_from_uri(file_uri, table_name, job_config=job_config)\n",
    "\n",
    "# wait to finish\n",
    "load_job.result() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTM_8ES_0Dyb"
   },
   "outputs": [],
   "source": [
    "! pip install google-cloud-storage --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Co2HY-m50GKa",
    "outputId": "df7499d2-a12c-4288-b6fd-08259a3dbfac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_CHUNK_SIZE_MULTIPLE', '_STORAGE_CLASSES', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_acl', '_changes', '_chunk_size', '_do_download', '_do_multipart_upload', '_do_resumable_upload', '_do_upload', '_encryption_headers', '_encryption_key', '_get_content_type', '_get_download_url', '_get_transport', '_get_upload_arguments', '_get_writable_metadata', '_initiate_resumable_upload', '_patch_property', '_properties', '_query_params', '_require_client', '_set_properties', 'acl', 'bucket', 'cache_control', 'chunk_size', 'client', 'component_count', 'compose', 'content_disposition', 'content_encoding', 'content_language', 'content_type', 'crc32c', 'create_resumable_upload_session', 'delete', 'download_as_string', 'download_to_file', 'download_to_filename', 'etag', 'event_based_hold', 'exists', 'generate_signed_url', 'generation', 'get_iam_policy', 'id', 'kms_key_name', 'make_private', 'make_public', 'md5_hash', 'media_link', 'metadata', 'metageneration', 'name', 'owner', 'patch', 'path', 'path_helper', 'public_url', 'reload', 'retention_expiration_time', 'rewrite', 'self_link', 'set_iam_policy', 'size', 'storage_class', 'temporary_hold', 'test_iam_permissions', 'time_created', 'time_deleted', 'update', 'update_storage_class', 'updated', 'upload_from_file', 'upload_from_filename', 'upload_from_string', 'user_project']\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "file_uri = \"gs://gcs_file_to_bq_new_cloud_func/AgentConnectionDetail_brazil_test.csv\"\n",
    "bucket_name = \"gcs_file_to_bq_new_cloud_func\"\n",
    "file_name = file_uri.split('/')[-1]\n",
    "archive_folder_name = 'archive'\n",
    "\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "blob = bucket.get_blob(file_name)\n",
    "\n",
    "# try:\n",
    "#   bucket.copy_blob(blob, bucket, new_name=\"{}/{}\".format(archive_folder_name, file_name))\n",
    "\n",
    "#   bucket.delete(blob)\n",
    "# except Exception as e:\n",
    "#   raise Exception(\"When to copy file from GCS to archive with error: {}\".format(e))\n",
    "print(dir(blob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oX_X9F6gH7AB",
    "outputId": "287f0908-ba37-443a-a95d-1b108664da74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "# let's make it into cloud function, cloud function has to be in main.py\n",
    "%%writefile main.py\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "\n",
    "def get_table_schema():\n",
    "  table_str = \"\"\"sessionid int64,\n",
    "    sessionseqnum int64,\n",
    "    nodeid int64,\n",
    "    profileid int64,\n",
    "    resourceid int64,\n",
    "    startdatetime timestamp,\n",
    "    enddatetime timestamp,\n",
    "    qindex int64,\n",
    "    gmtoffset int64,\n",
    "    ringtime int64,\n",
    "    talktime int64,\n",
    "    holdtime int64,\n",
    "    worktime int64,\n",
    "    callwrapupdata string,\n",
    "    callresult int64,\n",
    "    dialinglistid int64,\n",
    "    extractdatetime timestamp \"\"\"\n",
    "\n",
    "  columns_list = [x.replace('\\n', '').strip().replace('int64', 'integer').split(' ') for x in table_str.split(',')]\n",
    "\n",
    "  schema_list = []\n",
    "  for x in columns_list:\n",
    "    schema_list.append(bigquery.SchemaField(x[0], x[1]))\n",
    "  \n",
    "  return schema_list\n",
    "\n",
    "\n",
    "def move_gcs_file_into_archive(file_uri, archive_folder_name=\"archive\"):\n",
    "  \"\"\"\n",
    "  As the notification is based on bucket, so we have to move the files into archive folder, \n",
    "  then we could avoid re-run the load job again.\"\"\"\n",
    "  bucket_name = file_uri.split('/')[2]\n",
    "  file_name = file_uri.split('/')[-1]\n",
    "\n",
    "  bucket = storage_client.get_bucket(bucket_name)\n",
    "  blob = bucket.get_blob(file_name)\n",
    "\n",
    "  try:\n",
    "    bucket.copy_blob(blob, bucket, new_name=\"{}/{}\".format(archive_folder_name, file_name))\n",
    "\n",
    "    bucket.delete(blob)\n",
    "\n",
    "    print(\"File: {} has been moved into archive folder:{}\".format(file_name, archive_folder_name))\n",
    "  except Exception as e:\n",
    "    raise Exception(\"When to copy file from GCS to archive with error: {}\".format(e))\n",
    "\n",
    "  \n",
    "def load_file_into_bq(event, context):\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    table_name = \"uccx_22042_brazil.agent_connection_detail_dump\"\n",
    "    file_uri = \"gs://gcs_file_to_bq_new/AgentConnectionDetail_brazil_test.csv\"\n",
    "\n",
    "    schema_list = get_table_schema()\n",
    "\n",
    "    # config job config\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=schema_list,\n",
    "        skip_leading_rows=1,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        field_delimiter='|'\n",
    "    )\n",
    "\n",
    "    load_job = client.load_table_from_uri(file_uri, table_name, job_config=job_config)\n",
    "\n",
    "    # wait to finish\n",
    "    load_job.result() \n",
    "\n",
    "    print(\"start to do archive logic\")\n",
    "    move_gcs_file_into_archive(file_uri)\n",
    "\n",
    "    print(\"Load action has finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyH88FzEPRJv",
    "outputId": "84e462d8-88d6-4458-cd87-a5e6a88dba62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# have to add a requirements.txt file, otherwise couldn't find the package\n",
    "%%writefile requirements.txt\n",
    "google-cloud-bigquery\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mzesYxjKTId",
    "outputId": "51b87342-530a-4beb-9e98-82561d7008b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# set prioject for cloud function\n",
    "! gcloud config set project cloudtutorial-296001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HP4o_1dYKdck",
    "outputId": "6f35b181-65c8-4014-a6a1-74381835fd38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=26z7z1T4EU9SeaYsZDGupdsbqpQ6XJ&prompt=consent&access_type=offline&code_challenge=RoHacIGMApU_djOam9Qkz12ZNbdmbvypFEPfbMGh7Vw&code_challenge_method=S256\n",
      "\n",
      "Enter verification code: 4/1AY0e-g6WkrCBN7iAQOAfyzOhIs7DvlX0RCYWXoHjyekpQcIABjuiuvPOJkg\n",
      "\n",
      "You are now logged in as [gqianglu1990@gmail.com].\n",
      "Your current project is [cloudtutorial-296001].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "# need login\n",
    "! gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXXRqbPHJ7ks",
    "outputId": "b6e4db27-8983-4977-8e78-cdbc592b7041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "availableMemoryMb: 256\n",
      "buildId: efd6d87b-99e8-4a1c-8fe1-8aac4229eb65\n",
      "entryPoint: load_file_into_bq\n",
      "eventTrigger:\n",
      "  eventType: google.storage.object.finalize\n",
      "  failurePolicy: {}\n",
      "  resource: projects/_/buckets/gcs_file_to_bq_new\n",
      "  service: storage.googleapis.com\n",
      "ingressSettings: ALLOW_ALL\n",
      "labels:\n",
      "  deployment-tool: cli-gcloud\n",
      "name: projects/cloudtutorial-296001/locations/us-central1/functions/load_file_into_bq\n",
      "runtime: python37\n",
      "serviceAccountEmail: cloudtutorial-296001@appspot.gserviceaccount.com\n",
      "sourceUploadUrl: https://storage.googleapis.com/gcf-upload-us-central1-0d10e295-9b7c-4042-a1ec-11c8a01089fa/6b5a291e-4d2f-4e99-9169-ea6503a222ec.zip?GoogleAccessId=service-299658701686@gcf-admin-robot.iam.gserviceaccount.com&Expires=1605669701&Signature=gcLyQ1aVNogwC3Q1fZJXlO8FdRVGb0rrhQiB54n09TzH6PNLrdsGLxk77bD1cTmvggtqTJcqaeDVzRSsBNO62oaYRXsSC9ZguyNpbZA0PTDNRFMDji5PPSZS4EIu24cZuare8JySPDw%2Bp%2BzVWREgBy64Q%2BgGV22daEse3D5B%2BYY22vmiCw0LiwT2i73U49h38COtD3ayoktWzpsvdJct0pBBT4OrjTdrlSsDX3ECBQVb7Y4zlKjHRyrL9x9s5OXD4qUsKXDc7zi%2FSeaX68CS2o%2F%2F42atEx%2FgDUvTGdPvX4ZwM%2B5xF2entZS5W5n30fse8KCCPggXpZ87fK0q9aYDtA%3D%3D\n",
      "status: ACTIVE\n",
      "timeout: 60s\n",
      "updateTime: '2020-11-18T02:52:34.726Z'\n",
      "versionId: '8'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deploying function (may take a while - up to 2 minutes)...\n",
      "..\n",
      "For Cloud Build Stackdriver Logs, visit: https://console.cloud.google.com/logs/viewer?project=cloudtutorial-296001&advancedFilter=resource.type%3Dbuild%0Aresource.labels.build_id%3Defd6d87b-99e8-4a1c-8fe1-8aac4229eb65%0AlogName%3Dprojects%2Fcloudtutorial-296001%2Flogs%2Fcloudbuild\n",
      ".............................done.\n"
     ]
    }
   ],
   "source": [
    "# deploy the code\n",
    "%%bash\n",
    "gcloud functions deploy load_file_into_bq \\\n",
    "--runtime python37 \\\n",
    "--trigger-resource gcs_file_to_bq_new \\\n",
    "--trigger-event google.storage.object.finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFgDNgpbxRma"
   },
   "source": [
    "### Storage to pubsub then Cloud functions\n",
    "\n",
    "As we could make our cloud functions based on the change of the bucket, but couldn't based on file, so let's try to use **notification** based on storage to pubsub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EB0UW7NcKOeL",
    "outputId": "22a1020b-dccf-4c29-fadd-2b4059d3ee75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Cloud Pub/Sub topic projects/cloudtutorial-296001/topics/gcs-to-bq-cloud\n",
      "Created notification config projects/_/buckets/gcs_file_to_bq_new_cloud_func/notificationConfigs/1\n"
     ]
    }
   ],
   "source": [
    "# make notification\n",
    "! gsutil notification create -t gcs-to-bq-cloud -f json gs://gcs_file_to_bq_new_cloud_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnCbCRX9yBbo",
    "outputId": "a224664b-efa0-4b56-ef85-de8074e22b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "dataset_name = \"uccx_22042_brazil\"\n",
    "bucket_name = \"gcs_file_to_bq_new_cloud_func\"\n",
    "\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "\n",
    "def check_blob_exist_or_not(file_name):\n",
    "  blob = bucket.get_blob(file_name)\n",
    "\n",
    "  if not blob or not blob.exists():\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_table_schema():\n",
    "  table_str = \"\"\"sessionid int64,\n",
    "    sessionseqnum int64,\n",
    "    nodeid int64,\n",
    "    profileid int64,\n",
    "    resourceid int64,\n",
    "    startdatetime timestamp,\n",
    "    enddatetime timestamp,\n",
    "    qindex int64,\n",
    "    gmtoffset int64,\n",
    "    ringtime int64,\n",
    "    talktime int64,\n",
    "    holdtime int64,\n",
    "    worktime int64,\n",
    "    callwrapupdata string,\n",
    "    callresult int64,\n",
    "    dialinglistid int64,\n",
    "    extractdatetime timestamp \"\"\"\n",
    "\n",
    "  columns_list = [x.replace('\\n', '').strip().replace('int64', 'integer').split(' ') for x in table_str.split(',')]\n",
    "\n",
    "  schema_list = []\n",
    "  for x in columns_list:\n",
    "    schema_list.append(bigquery.SchemaField(x[0], x[1]))\n",
    "  \n",
    "  return schema_list\n",
    "\n",
    "\n",
    "def move_gcs_file_into_archive(file_uri, archive_folder_name=\"archive\"):\n",
    "  \"\"\"\n",
    "  As the notification is based on bucket, so we have to move the files into archive folder, \n",
    "  then we could avoid re-run the load job again.\"\"\"\n",
    "  bucket_name = file_uri.split('/')[2]\n",
    "  file_name = file_uri.split('/')[-1]\n",
    "\n",
    "  if not check_blob_exist_or_not(file_name):\n",
    "    print(\"When try to use movement logic, File not exist\")\n",
    "    return None\n",
    "\n",
    "  bucket = storage_client.get_bucket(bucket_name)\n",
    "  blob = bucket.get_blob(file_name)\n",
    "\n",
    "  try:\n",
    "    # actually here will still trigger another message, as there are modification in the bucket, also\n",
    "    # with delete...\n",
    "    bucket.copy_blob(blob, bucket, new_name=\"{}/{}\".format(archive_folder_name, file_name))\n",
    "\n",
    "    bucket.delete_blob(file_name)\n",
    "\n",
    "    print(\"File: {} has been moved into archive folder:{}\".format(file_name, archive_folder_name))\n",
    "  except Exception as e:\n",
    "    raise Exception(\"When to copy file from GCS to archive with error: {}\".format(e))\n",
    "\n",
    "  \n",
    "def load_file_into_bq(file_name, table_name):\n",
    "    table_name = \"{}.{}\".format(dataset_name, table_name)\n",
    "    file_uri = \"gs://{}/{}\".format(bucket_name, file_name)\n",
    "\n",
    "    # This is to check the file exist in the bucket or not.\n",
    "    if not check_blob_exist_or_not(file_name):\n",
    "      print(\"We don't find the file, so couldn't load file into BQ!\")\n",
    "      return \"We don't find the file, so couldn't load file into BQ\"\n",
    "\n",
    "    schema_list = get_table_schema()\n",
    "\n",
    "    # config job config\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=schema_list,\n",
    "        skip_leading_rows=1,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        field_delimiter='|'\n",
    "    )\n",
    "\n",
    "    print(\"Get file uri:\", file_uri)\n",
    "\n",
    "    load_job = bigquery_client.load_table_from_uri(file_uri, table_name, job_config=job_config)\n",
    "\n",
    "    # wait to finish\n",
    "    load_result = load_job.result() \n",
    "\n",
    "    if load_result.state == 'DONE':\n",
    "      print(\"start to do archive logic AS load has finished\")\n",
    "      move_gcs_file_into_archive(file_uri)\n",
    "\n",
    "      print(\"Load action has finished\")\n",
    "\n",
    "\n",
    "def sub_storage_pub(event, context):\n",
    "  \"\"\"event is a dict\"\"\"\n",
    "  print(\"Function is triggerred by messageID:{} published at {}\".format(context.event_id, context.timestamp))\n",
    "  # print(\"Get event data: \", json.dumps(event))\n",
    "\n",
    "  if \"data\" in event:\n",
    "    # name = base64.b64decode(event['data']).decode('utf-8')\n",
    "    event_data = json.loads(base64.b64decode(event['data']).decode('utf-8'))\n",
    "    file_name = event_data['name']\n",
    "    if file_name.startswith('AgentConnectionDetail_brazil'):\n",
    "      load_file_into_bq(file_name=\"AgentConnectionDetail_brazil_test.csv\", table_name=\"agent_connection_detail_dump\")\n",
    "\n",
    "    if file_name.startswith(\"AgentConnectionDetail_india\"):\n",
    "      load_file_into_bq(file_name=\"AgentConnectionDetail_india.csv\", table_name=\"agent_connection_detail_dump\")\n",
    "\n",
    "    else:\n",
    "      print(\"We don't find statisfy files.\")\n",
    "  else:\n",
    "    print(\"Not good message.\")\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7NYnP_yg0io",
    "outputId": "a3e57321-1c8b-4d4a-90eb-9b58519121bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "availableMemoryMb: 256\n",
      "buildId: d3833476-8877-4fde-9638-568ea27a707c\n",
      "entryPoint: sub_storage_pub\n",
      "eventTrigger:\n",
      "  eventType: google.pubsub.topic.publish\n",
      "  failurePolicy: {}\n",
      "  resource: projects/cloudtutorial-296001/topics/gcs-to-bq-cloud\n",
      "  service: pubsub.googleapis.com\n",
      "ingressSettings: ALLOW_ALL\n",
      "labels:\n",
      "  deployment-tool: cli-gcloud\n",
      "name: projects/cloudtutorial-296001/locations/us-central1/functions/sub_storage_pub\n",
      "runtime: python37\n",
      "serviceAccountEmail: cloudtutorial-296001@appspot.gserviceaccount.com\n",
      "sourceUploadUrl: https://storage.googleapis.com/gcf-upload-us-central1-0d10e295-9b7c-4042-a1ec-11c8a01089fa/079e7cde-e116-4134-87bc-460eb83490ac.zip?GoogleAccessId=service-299658701686@gcf-admin-robot.iam.gserviceaccount.com&Expires=1605841801&Signature=CscOiVaH35RYJJfL1SKoeGaSWSn%2F7jwWpnucp%2BnLDh50U8yII5wuCybVAvShT3O%2F%2F7wK9%2FZIphMdELwJWiKm%2F4D8rbehwU7cayktqivVhCuJMcc8lJC9FEO%2FPpn8wRuZZyfCDFCjvov1OI7%2B1JH%2BYTxfQ1edjVTg4BxKp5b1tY4fBn4ZlIBhGHzrSVx8HDbAlZcQmzCSfm3%2BLdD%2FFuOC0COwFbCwBALdTEzyMMpTmWxWKGzOPnBPGAxOtjBmxIupDb08NfCzwbA%2BOldadlFx7KBqVOYFdLI4y3Q8NXH4E1wSqbXBWC4Gy851R0OsTkphbs9ScPesZrfveaMnRHGjxQ%3D%3D\n",
      "status: ACTIVE\n",
      "timeout: 60s\n",
      "updateTime: '2020-11-20T02:41:06.561Z'\n",
      "versionId: '11'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deploying function (may take a while - up to 2 minutes)...\n",
      "..\n",
      "For Cloud Build Stackdriver Logs, visit: https://console.cloud.google.com/logs/viewer?project=cloudtutorial-296001&advancedFilter=resource.type%3Dbuild%0Aresource.labels.build_id%3Dd3833476-8877-4fde-9638-568ea27a707c%0AlogName%3Dprojects%2Fcloudtutorial-296001%2Flogs%2Fcloudbuild\n",
      "......................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud functions deploy sub_storage_pub \\\n",
    "--runtime python37 \\\n",
    "--trigger-topic gcs-to-bq-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zcsqtz3Zyod",
    "outputId": "337aa8f1-9e9f-4d8d-aef7-b7643e2a10da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://AgentConnectionDetail_brazil_test.csv [Content-Type=text/csv]...\n",
      "Copying file://AgentConnectionDetail_india.csv [Content-Type=text/csv]...\n",
      "-\n",
      "Operation completed over 2 objects/22.2 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp *.csv gs://gcs_file_to_bq_new_cloud_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doYuisgpkl2o"
   },
   "source": [
    "### Cloud function with **PUSH** logic by http trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1FW0S5ugaNX",
    "outputId": "c67604e5-b722-4e7f-b32e-bdbf02eee64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import json\n",
    "\n",
    "def hello_pubsub(request):\n",
    "    import base64\n",
    "\n",
    "    # print(\"\"\"This Function was triggered by messageId {} published at {}\n",
    "    # \"\"\".format(context.event_id, context.timestamp))\n",
    "    request_data = request.get_json(silent=True)\n",
    "    request_args = request.args\n",
    "\n",
    "    print(\"Get requst:\", request_data)\n",
    "    if not request_data:\n",
    "      request_data = request.form\n",
    "\n",
    "    # here to add data extraction logic from pubsub\n",
    "    if 'data' in request_data['message']:\n",
    "      event_data = json.loads(base64.b64decode(request_data['message']['data']).decode('utf-8'))\n",
    "      name = event_data['name']\n",
    "      print(\"Get name\", name)\n",
    "    else:\n",
    "      name = 'world'\n",
    "\n",
    "    # if request_data and 'name' in request_data:\n",
    "    #   print(\"Now is json data\")\n",
    "    #   name = request_data['name']\n",
    "    # elif request_args and 'name' in request_args:\n",
    "    #   name = request_args['name']\n",
    "    # else:\n",
    "    #     name = 'World'\n",
    "\n",
    "    print('Hello {}!'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEYL2FbRk9Ih",
    "outputId": "1f0db04a-e8eb-4b62-fe56-5f2e63a58239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Cloud Build Stackdriver Logs, visit: https://console.cloud.google.com/logs/viewer?project=cloudtutorial-296001&advancedFilter=resource.type%3Dbuild%0Aresource.labels.build_id%3Da0f5d0c8-79b3-448d-b372-478eda0d1774%0AlogName%3Dprojects%2Fcloudtutorial-296001%2Flogs%2Fcloudbuild\n",
      "availableMemoryMb: 256\n",
      "buildId: a0f5d0c8-79b3-448d-b372-478eda0d1774\n",
      "entryPoint: hello_pubsub\n",
      "httpsTrigger:\n",
      "  url: https://us-central1-cloudtutorial-296001.cloudfunctions.net/hello_pubsub\n",
      "ingressSettings: ALLOW_ALL\n",
      "labels:\n",
      "  deployment-tool: cli-gcloud\n",
      "name: projects/cloudtutorial-296001/locations/us-central1/functions/hello_pubsub\n",
      "runtime: python37\n",
      "serviceAccountEmail: cloudtutorial-296001@appspot.gserviceaccount.com\n",
      "sourceUploadUrl: https://storage.googleapis.com/gcf-upload-us-central1-0d10e295-9b7c-4042-a1ec-11c8a01089fa/5c82f515-a3a5-479f-928c-e65d6b5d809b.zip?GoogleAccessId=service-299658701686@gcf-admin-robot.iam.gserviceaccount.com&Expires=1605854283&Signature=dkykeBRA0heVbd2%2BDe8o9gvEnHC%2BNPUgOZr8056qLmP5nUfe3pYJW2VuGKz5OxUnAdBERYABqlwFfuE95qYO7ZVe2UqdNwhbouaR2xi2IHBZfvlFJyO1Nqu7UI7JWvjQxHIbWDjvCeRY0LcSYm6ejVhDMrBDE5JfG4LGil026CEMycYtxEal9N1Q%2FNebbUaUyemvMFKOMw%2FdSqQMUezCMIvfOdD8yPHz70sTI4p%2FQs6gcb0zxmYGBgoCaoAOVEsizS87HoxaGdcQGNwgMXqkGuxebJpn1QL1BWTF%2BNgVtgrnSx3LjD%2F%2BP70YY%2FOxWE5sD17h7kiK1L08WZyawQFFiQ%3D%3D\n",
      "status: ACTIVE\n",
      "timeout: 60s\n",
      "updateTime: '2020-11-20T06:09:15.756Z'\n",
      "versionId: '11'\n"
     ]
    }
   ],
   "source": [
    "! gcloud functions deploy hello_pubsub --trigger-http --runtime python37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSAR6gqolNFK",
    "outputId": "baa6dc88-c030-4d93-ccc4-1fae80a9f5c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100    17  100     2  100    15     11     87 --:--:-- --:--:-- --:--:--   100\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \"https://us-central1-cloudtutorial-296001.cloudfunctions.net/hello_pubsub\" -H \"Content-Type:application/json\" --data '{\"name\":\"lugq\"}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpJLDDB9C3Iu",
    "outputId": "4cd8fe9d-e9e8-498d-cd59-945c3bc3b7d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a post to our cloud function\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://us-central1-cloudtutorial-296001.cloudfunctions.net/hello_pubsub\"\n",
    "\n",
    "data = {\"name\": 'lugq'}\n",
    "\n",
    "requests.post(url, data={\"name\": \"lugq\"})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Load GCS files into Bigquery tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}