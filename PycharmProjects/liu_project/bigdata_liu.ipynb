{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import load_iris\n",
    "import warnings\n",
    "import tempfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_iris(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.reshape(-1, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((x, y),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c    d  label\n",
       "0  5.1  3.5  1.4  0.2    0.0\n",
       "1  4.9  3.0  1.4  0.2    0.0\n",
       "2  4.7  3.2  1.3  0.2    0.0\n",
       "3  4.6  3.1  1.5  0.2    0.0\n",
       "4  5.0  3.6  1.4  0.2    0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns= ['a', 'b', 'c', 'd', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(tmp_path, 'data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C:\\\\Users\\\\GUANGQ~1.LU\\\\AppData\\\\Local\\\\Temp\\\\tmpo8vtcztq, files:['data.csv']\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{}, files:{}'.format(tmp_path, os.listdir(tmp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.format('csv').option('header', True).option('inferSchema', True).load(os.path.join(tmp_path, 'data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+\n",
      "|  a|  b|  c|  d|label|\n",
      "+---+---+---+---+-----+\n",
      "|5.1|3.5|1.4|0.2|  0.0|\n",
      "|4.9|3.0|1.4|0.2|  0.0|\n",
      "|4.7|3.2|1.3|0.2|  0.0|\n",
      "|4.6|3.1|1.5|0.2|  0.0|\n",
      "|5.0|3.6|1.4|0.2|  0.0|\n",
      "|5.4|3.9|1.7|0.4|  0.0|\n",
      "|4.6|3.4|1.4|0.3|  0.0|\n",
      "|5.0|3.4|1.5|0.2|  0.0|\n",
      "|4.4|2.9|1.4|0.2|  0.0|\n",
      "|4.9|3.1|1.5|0.1|  0.0|\n",
      "|5.4|3.7|1.5|0.2|  0.0|\n",
      "|4.8|3.4|1.6|0.2|  0.0|\n",
      "|4.8|3.0|1.4|0.1|  0.0|\n",
      "|4.3|3.0|1.1|0.1|  0.0|\n",
      "|5.8|4.0|1.2|0.2|  0.0|\n",
      "|5.7|4.4|1.5|0.4|  0.0|\n",
      "|5.4|3.9|1.3|0.4|  0.0|\n",
      "|5.1|3.5|1.4|0.3|  0.0|\n",
      "|5.7|3.8|1.7|0.3|  0.0|\n",
      "|5.1|3.8|1.5|0.3|  0.0|\n",
      "+---+---+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: double (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: double (nullable = true)\n",
      " |-- d: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['a'], outputCol = 'a_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: double (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: double (nullable = true)\n",
      " |-- d: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- a_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_vector = assembler.transform(df_spark)\n",
    "df_spark_vector.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+--------+\n",
      "|  a|  b|  c|  d|label|a_vector|\n",
      "+---+---+---+---+-----+--------+\n",
      "|5.1|3.5|1.4|0.2|  0.0|   [5.1]|\n",
      "|4.9|3.0|1.4|0.2|  0.0|   [4.9]|\n",
      "|4.7|3.2|1.3|0.2|  0.0|   [4.7]|\n",
      "|4.6|3.1|1.5|0.2|  0.0|   [4.6]|\n",
      "|5.0|3.6|1.4|0.2|  0.0|   [5.0]|\n",
      "+---+---+---+---+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_vector.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol = 'a_vector', outputCol = 'a_standard', withMean = True, withStd= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = scaler.fit(df_spark_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaler = scaler_model.transform(df_spark_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+--------+---------------------+\n",
      "|a  |b  |c  |d  |label|a_vector|a_standard           |\n",
      "+---+---+---+---+-----+--------+---------------------+\n",
      "|5.1|3.5|1.4|0.2|0.0  |[5.1]   |[-0.8976738791967643]|\n",
      "|4.9|3.0|1.4|0.2|0.0  |[4.9]   |[-1.1392004834649512]|\n",
      "|4.7|3.2|1.3|0.2|0.0  |[4.7]   |[-1.3807270877331392]|\n",
      "|4.6|3.1|1.5|0.2|0.0  |[4.6]   |[-1.5014903898672336]|\n",
      "|5.0|3.6|1.4|0.2|0.0  |[5.0]   |[-1.0184371813308577]|\n",
      "+---+---+---+---+-----+--------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scaler.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bin(data):\n",
    "    if data>=1.5:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_udf = udf(convert_bin, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaler_udf = df_scaler.withColumn('c_bins', convert_udf(df_scaler['c']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+--------+---------------------+------+\n",
      "|a  |b  |c  |d  |label|a_vector|a_standard           |c_bins|\n",
      "+---+---+---+---+-----+--------+---------------------+------+\n",
      "|5.1|3.5|1.4|0.2|0.0  |[5.1]   |[-0.8976738791967643]|0.0   |\n",
      "|4.9|3.0|1.4|0.2|0.0  |[4.9]   |[-1.1392004834649512]|0.0   |\n",
      "|4.7|3.2|1.3|0.2|0.0  |[4.7]   |[-1.3807270877331392]|0.0   |\n",
      "|4.6|3.1|1.5|0.2|0.0  |[4.6]   |[-1.5014903898672336]|1.0   |\n",
      "|5.0|3.6|1.4|0.2|0.0  |[5.0]   |[-1.0184371813308577]|0.0   |\n",
      "+---+---+---+---+-----+--------+---------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scaler_udf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_ml = VectorAssembler(inputCols = ['a', 'b', 'c', 'd', 'a_standard', 'c_bins'], outputCol= 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = assembler_ml.transform(df_scaler_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+--------+---------------------+------+-----------------------------------------+\n",
      "|a  |b  |c  |d  |label|a_vector|a_standard           |c_bins|features                                 |\n",
      "+---+---+---+---+-----+--------+---------------------+------+-----------------------------------------+\n",
      "|5.1|3.5|1.4|0.2|0.0  |[5.1]   |[-0.8976738791967643]|0.0   |[5.1,3.5,1.4,0.2,-0.8976738791967643,0.0]|\n",
      "|4.9|3.0|1.4|0.2|0.0  |[4.9]   |[-1.1392004834649512]|0.0   |[4.9,3.0,1.4,0.2,-1.1392004834649512,0.0]|\n",
      "|4.7|3.2|1.3|0.2|0.0  |[4.7]   |[-1.3807270877331392]|0.0   |[4.7,3.2,1.3,0.2,-1.3807270877331392,0.0]|\n",
      "|4.6|3.1|1.5|0.2|0.0  |[4.6]   |[-1.5014903898672336]|1.0   |[4.6,3.1,1.5,0.2,-1.5014903898672336,1.0]|\n",
      "|5.0|3.6|1.4|0.2|0.0  |[5.0]   |[-1.0184371813308577]|0.0   |[5.0,3.6,1.4,0.2,-1.0184371813308577,0.0]|\n",
      "+---+---+---+---+-----+--------+---------------------+------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_selected = df_train.select(['features', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = df_train_selected.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=50, regParam = 0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+----------------------------------------------------------------+-----------------------------------------------------------+----------+\n",
      "|features                                 |label|rawPrediction                                                   |probability                                                |prediction|\n",
      "+-----------------------------------------+-----+----------------------------------------------------------------+-----------------------------------------------------------+----------+\n",
      "|[4.4,3.0,1.3,0.2,-1.7430169941354205,0.0]|0.0  |[0.553290699802792,-0.22434544395498854,-0.5918681350258309]    |[0.5625359150491913,0.2584801333091901,0.17898395164161865]|0.0       |\n",
      "|[4.9,3.0,1.4,0.2,-1.1392004834649512,0.0]|0.0  |[0.5314930920943604,-0.22434544395498854,-0.5918681350258309]   |[0.557164648832679,0.2616538009437328,0.18118155022358817] |0.0       |\n",
      "|[5.0,3.5,1.3,0.3,-1.0184371813308577,0.0]|0.0  |[0.5245005712700129,-0.22434544395498854,-0.5528549959772608]   |[0.5514482699431265,0.2607864883496986,0.18776524170717507]|0.0       |\n",
      "|[5.1,2.5,3.0,1.1,-0.8976738791967643,1.0]|1.0  |[-0.07637978803555501,-0.22434544395498854,-0.24074988358869948]|[0.3688827211139235,0.3181468627269946,0.31297041615908183]|0.0       |\n",
      "|[5.1,3.7,1.5,0.4,-0.8976738791967643,1.0]|0.0  |[0.4521152273203709,-0.22434544395498854,-0.5138418569286907]   |[0.5293716585594742,0.2691395671557881,0.20148877428473763]|0.0       |\n",
      "+-----------------------------------------+-----+----------------------------------------------------------------+-----------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
