{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Spark\n",
    "\n",
    "I think you do hear `big data` these years, `big data` is just like traditional technologies but just data is too big that we couldn't use common tools to process it, like if you have 10,000 records data, you could just store it in MYSQL,  but if you have 100000000 records data, if have just store them in MYSQL, it couldn't handle it in fact, that's where big data comes in, there are many tools that we could use for `big data`,  but most common tools are `Spark` to process data, `HIVE` to store relational data, `HDFS` for storing distributed files(What means `distributed`, for big data, we couldn't just use one machine,but many machines, commonly one machine as `master` machine, the others are `workers` machines), so for these `big data` technologies are both distributed.\n",
    "\n",
    "As there are many tools that we could use, but most popular tool is [`Spark`](https://spark.apache.org/), it's a unified analytics engine for large-scale data processing, it combines many tools like `SQL`, `Machine learning`, `streaming`, this notebook is try to show you how to use `Spark` to do machine learning also with `SQL`.\n",
    "\n",
    "Let's get start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before do anything, maybe you should first install `pyspark` module for python\n",
    "# just use pip command:   pip install pyspark\n",
    "\n",
    "# if you installed the pyspark, but you couldn't import it, that's means you should install java also,\n",
    "# if you don't know how to do that, I will show you with your computer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import load_iris\n",
    "import warnings\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# let's try to import pyspark module, \n",
    "import pyspark\n",
    "\n",
    "# then let's try to import SparkSession, as spark is a distributed logic, we do need a entry for \n",
    "# code to start to run, that's SparkSession, if you are not familiar with it, never mind, just keep it in mind\n",
    "from pyspark.sql import SparkSession\n",
    "warnings.simplefilter('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# let's first to check what pyspark version we use\n",
    "# so currently we use 2.3.2 version.\n",
    "print(\"Spark version:\", pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data into a file\n",
    "\n",
    "first let's try to store the iris data into a file, then we could use spark to read it, one thing to notice is that we use spark most time is for big data means that maybe the data is stored in distributed file system like `HDFS`, don't worry about `HDFS`, that's easy to use and understand, you could just take it as a system to store files(files means even for MYSQL, data is stored as a file or many files), but currently I just to make it into a local server, if in HDFS could just change one line code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is stored in path: C:\\Users\\GUANGQ~1.LU\\AppData\\Local\\Temp\\tmp_8puox6t, files: ['data.csv']\n"
     ]
    }
   ],
   "source": [
    "# first make a temp folder to store data file\n",
    "tmp_path = tempfile.mkdtemp()\n",
    "\n",
    "# currently I just to make the data into a file, in real project, they are just stored in somewhere.\n",
    "x, y = load_iris(return_X_y=True)\n",
    "data = np.concatenate([x, y.reshape(-1, 1)], axis=1)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['a', 'b', 'c', 'd', 'label'])\n",
    "\n",
    "# let's store the dataframe into a csv file\n",
    "df.to_csv(os.path.join(tmp_path, 'data.csv'), index=False)   # we don't need index\n",
    "\n",
    "# let's check, you could see that we have stored data file into tmp folder.\n",
    "print(\"What is stored in path: {}, files: {}\".format(tmp_path, os.listdir(tmp_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data(ETL step)\n",
    "\n",
    "let's use spark to read it from local server, one thing to notice is that Spark also use `DataFrame`, that's it just like `pandas`'s dataframe, but spark's returned `DataFrame` is a distributed object. If you are not famaliar about `distributed` dataframe, I could just explain it with you less than 5 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we do anything, we should first init `SparkSession`, so that we could interact with `distributed` system\n",
    "# it maybe takes some time\n",
    "spark = SparkSession.builder.getOrCreate()     # most times should just like this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we have get sparksesion object, then we could use it to read data\n",
    "# these two options:`.option('header', True).option('inferSchema', True)` is to keep the csv header for new dataframe.\n",
    "\n",
    "df_spark = spark.read.format('csv').option('header', True).option('inferSchema', True).load(os.path.join(tmp_path, 'data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check it, what it is: so you could see that it's `pyspark.sql.dataframe.DataFrame`, that's a DataFrame\n",
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+\n",
      "|  a|  b|  c|  d|label|\n",
      "+---+---+---+---+-----+\n",
      "|5.1|3.5|1.4|0.2|  0.0|\n",
      "|4.9|3.0|1.4|0.2|  0.0|\n",
      "|4.7|3.2|1.3|0.2|  0.0|\n",
      "|4.6|3.1|1.5|0.2|  0.0|\n",
      "|5.0|3.6|1.4|0.2|  0.0|\n",
      "|5.4|3.9|1.7|0.4|  0.0|\n",
      "|4.6|3.4|1.4|0.3|  0.0|\n",
      "|5.0|3.4|1.5|0.2|  0.0|\n",
      "|4.4|2.9|1.4|0.2|  0.0|\n",
      "|4.9|3.1|1.5|0.1|  0.0|\n",
      "|5.4|3.7|1.5|0.2|  0.0|\n",
      "|4.8|3.4|1.6|0.2|  0.0|\n",
      "|4.8|3.0|1.4|0.1|  0.0|\n",
      "|4.3|3.0|1.1|0.1|  0.0|\n",
      "|5.8|4.0|1.2|0.2|  0.0|\n",
      "|5.7|4.4|1.5|0.4|  0.0|\n",
      "|5.4|3.9|1.3|0.4|  0.0|\n",
      "|5.1|3.5|1.4|0.3|  0.0|\n",
      "|5.7|3.8|1.7|0.3|  0.0|\n",
      "|5.1|3.8|1.5|0.3|  0.0|\n",
      "+---+---+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check what is contains\n",
    "# most time is `show` function, it will show 20 rows by default\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL follow \n",
    "\n",
    "After we have already get dataframe, then we could do other transformations aka `ETL`, what we could do with pandas dataframe, we could do it by spark.\n",
    "\n",
    "Let's show you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: double (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: double (nullable = true)\n",
      " |-- d: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check spark dataframe columns type\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice is that if we want to use `machine learning`, most functions assume each column is a `Vector` type, currently we just get `double` type. \n",
    "\n",
    "Let's convert `double` type into `vector` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+--------+\n",
      "|  a|  b|  c|  d|label|a_vector|\n",
      "+---+---+---+---+-----+--------+\n",
      "|5.1|3.5|1.4|0.2|  0.0|   [5.1]|\n",
      "|4.9|3.0|1.4|0.2|  0.0|   [4.9]|\n",
      "|4.7|3.2|1.3|0.2|  0.0|   [4.7]|\n",
      "|4.6|3.1|1.5|0.2|  0.0|   [4.6]|\n",
      "|5.0|3.6|1.4|0.2|  0.0|   [5.0]|\n",
      "+---+---+---+---+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['a'], outputCol='a_vector')\n",
    "\n",
    "# we don't need to fit the `VectorAssembler`, we could just use it to transform\n",
    "df_spark_vector = assembler.transform(df_spark)\n",
    "\n",
    "# let's check the new dataframe, so we could see that we do add a new column\n",
    "# called `a_vector`, right?\n",
    "df_spark_vector.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+--------+---------------------+\n",
      "|a  |b  |c  |d  |label|a_vector|a_standard           |\n",
      "+---+---+---+---+-----+--------+---------------------+\n",
      "|5.1|3.5|1.4|0.2|0.0  |[5.1]   |[-0.8976738791967643]|\n",
      "|4.9|3.0|1.4|0.2|0.0  |[4.9]   |[-1.1392004834649512]|\n",
      "|4.7|3.2|1.3|0.2|0.0  |[4.7]   |[-1.3807270877331392]|\n",
      "|4.6|3.1|1.5|0.2|0.0  |[4.6]   |[-1.5014903898672336]|\n",
      "|5.0|3.6|1.4|0.2|0.0  |[5.0]   |[-1.0184371813308577]|\n",
      "+---+---+---+---+-----+--------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# maybe we want to standard the `a` column, we could use spark functions to do it\n",
    "# for standard belongs to `machine learning`, so it is in `ml` package.\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# we have to define which column to process and define the output column.\n",
    "scaler = StandardScaler(inputCol='a_vector', outputCol='a_standard', withMean=True, withStd=True)\n",
    "\n",
    "# this is to fit model, returned `scaler_model` is the trained model based on the new dataframe: `df_spark_vector`\n",
    "scaler_model = scaler.fit(df_spark_vector)\n",
    "\n",
    "# let's use the trained model to get transform data\n",
    "df_scaler = scaler_model.transform(df_spark_vector)\n",
    "\n",
    "# let's check the new dataframe, we want to see whole columns, we could set truncate to false,\n",
    "# then we could see whole things in columns.\n",
    "df_scaler.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced function\n",
    "\n",
    "Sometimes, we would like to use our logic to process data as we want, then we would need to use spark `udf` function(udf means user defined function), but if we learn deeper, then I could tell you how to use it in real project, but for now you could just see that we could do that with spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+--------+---------------------+------+\n",
      "|a  |b  |c  |d  |label|a_vector|a_standard           |c_bins|\n",
      "+---+---+---+---+-----+--------+---------------------+------+\n",
      "|5.1|3.5|1.4|0.2|0.0  |[5.1]   |[-0.8976738791967643]|0.0   |\n",
      "|4.9|3.0|1.4|0.2|0.0  |[4.9]   |[-1.1392004834649512]|0.0   |\n",
      "|4.7|3.2|1.3|0.2|0.0  |[4.7]   |[-1.3807270877331392]|0.0   |\n",
      "|4.6|3.1|1.5|0.2|0.0  |[4.6]   |[-1.5014903898672336]|1.0   |\n",
      "|5.0|3.6|1.4|0.2|0.0  |[5.0]   |[-1.0184371813308577]|0.0   |\n",
      "+---+---+---+---+-----+--------+---------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# before we go on, we could also use ourself defined function to do transformations\n",
    "# let's show you. but we need to use udf logic in spark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# this is the main logic that we want to process for each row.\n",
    "def convert_bin(data):\n",
    "    if data >= 1.5:\n",
    "        return 1.0\n",
    "    else: \n",
    "        return 0.0\n",
    "\n",
    "# this is to register a `udf` function.\n",
    "convert_udf = udf(convert_bin, DoubleType())\n",
    "\n",
    "# we could use our defined function to do the transformation with lambda function\n",
    "df_scaler_udf = df_scaler.withColumn('c_bins', convert_udf(df_scaler[\"c\"]))\n",
    "\n",
    "# let's check the result\n",
    "df_scaler_udf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning with Spark\n",
    "\n",
    "Alright, we have seen that we could use spark to do `ETL`, most times, I do use `Spark` many times to do `ETL` and `machine learning`, I will show you how to do machine learning training with `Spark`, stay tuned, let's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+--------+---------------------+------+-----------------------------------------+\n",
      "|a  |b  |c  |d  |label|a_vector|a_standard           |c_bins|features                                 |\n",
      "+---+---+---+---+-----+--------+---------------------+------+-----------------------------------------+\n",
      "|5.1|3.5|1.4|0.2|0.0  |[5.1]   |[-0.8976738791967643]|0.0   |[5.1,3.5,1.4,0.2,-0.8976738791967643,0.0]|\n",
      "|4.9|3.0|1.4|0.2|0.0  |[4.9]   |[-1.1392004834649512]|0.0   |[4.9,3.0,1.4,0.2,-1.1392004834649512,0.0]|\n",
      "|4.7|3.2|1.3|0.2|0.0  |[4.7]   |[-1.3807270877331392]|0.0   |[4.7,3.2,1.3,0.2,-1.3807270877331392,0.0]|\n",
      "|4.6|3.1|1.5|0.2|0.0  |[4.6]   |[-1.5014903898672336]|1.0   |[4.6,3.1,1.5,0.2,-1.5014903898672336,1.0]|\n",
      "|5.0|3.6|1.4|0.2|0.0  |[5.0]   |[-1.0184371813308577]|0.0   |[5.0,3.6,1.4,0.2,-1.0184371813308577,0.0]|\n",
      "+---+---+---+---+-----+--------+---------------------+------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# before we do training, we need to make spark dataframe columns into a vector type for training, just like `a_vector` logic\n",
    "\n",
    "# most times, I would make the whole features that we need into one column called: `features`, you should do that either.\n",
    "assembler_ml = VectorAssembler(inputCols=['a', 'b', 'c', 'd', 'a_standard', 'c_bins'], outputCol='features')\n",
    "\n",
    "df_train = assembler_ml.transform(df_scaler_udf)\n",
    "\n",
    "# let's last check with the trained dataframe\n",
    "# we would just use the last column: `features` and 'label' for training. \n",
    "df_train.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many training:  125\n",
      "How many testing:  25\n"
     ]
    }
   ],
   "source": [
    "# before we do training, we need to split data into train and test data\n",
    "# we could first just to select the columns that we need: `features` and `label`\n",
    "df_train_selected = df_train.select(['features', 'label'])\n",
    "\n",
    "# then we could split it into train and test data with 8:2\n",
    "(train_data, test_data) = df_train_selected.randomSplit([0.8, 0.2])\n",
    "\n",
    "print(\"How many training: \", train_data.count())\n",
    "print(\"How many testing: \", test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model traning finished.\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[4.4,3.0,1.3,0.2,...|  0.0|[0.51850706598155...|[0.53994585442645...|       0.0|\n",
      "|[4.6,3.1,1.5,0.2,...|  0.0|[0.48105142290053...|[0.53062885971303...|       0.0|\n",
      "|[4.7,3.2,1.3,0.2,...|  0.0|[0.52140163258939...|[0.54066479367159...|       0.0|\n",
      "|[4.9,3.6,1.4,0.1,...|  0.0|[0.53760753814345...|[0.54830337835601...|       0.0|\n",
      "|[5.0,3.3,1.4,0.2,...|  0.0|[0.50339745270084...|[0.53619036584358...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's start with machine learning with Logistic Regression in Spark\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# when to init the object, we could set both `l1` and `l2` regularization.\n",
    "lr = LogisticRegression(maxIter=50, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# let's start our training with training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "print(\"Model traning finished.\")\n",
    "\n",
    "# let's get the prediction on test data: for spark framework, the prediction function is `transform`\n",
    "pred = lr_model.transform(test_data)\n",
    "\n",
    "# let's check what's like for test data\n",
    "# for probability is the probability of each class, you could just focus on `probability` and `prediction` column.\n",
    "pred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we have get prediction result, let's check the accuracy of our model with spark\n",
    "# for spark, we need to convert dataframe into RDD, so that we could directly eveluate prediction\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# just get the `label` and `prediction` columns.\n",
    "pred_evaluate = pred.select(['label', 'prediction']).rdd\n",
    "\n",
    "metrics = MulticlassMetrics(pred_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy on test data: 84.0000 %\n"
     ]
    }
   ],
   "source": [
    "print(\"model accuracy on test data: {:.4f} %\".format(metrics.accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model recall score: 0.84\n",
      "Model precision score: 0.84\n",
      "Model f1 score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# in fact, we could also get some other metrics of the prediction result based on\n",
    "# the metrics object, like recall, precision, f2-score etc.\n",
    "\n",
    "recall_score = metrics.recall()\n",
    "precision_score = metrics.precision()\n",
    "f1_score = metrics.fMeasure()\n",
    "print(\"Model recall score:\", recall_score)\n",
    "print(\"Model precision score:\", precision_score)\n",
    "print(\"Model f1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 2.0]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_label = df_train_selected.rdd.map(lambda x: x.label).distinct().collect()\n",
    "unique_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last words\n",
    "\n",
    "Currently I just walk through of `Spark` that if you are not familiar with `big data` technologies, I have to say that for `Spark` is really good to use in real project. One thing to notice is that I just try to let you know what is `Spark` and how to use it, there are many other ways that we could use `Spark`, but if we do face the problem that could use `Spark` to process data like `ETL` or `machine learning`, I will try to guide you.\n",
    "\n",
    "\n",
    "Hope you have a good understanding about `Spark` for `Big data`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
