{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯实现\n",
    "\n",
    "贝叶斯是一个概率模型。\n",
    "\n",
    "贝叶斯的目标是最小化目标函数\n",
    "$h(x) = arg min  R(c|x)$\n",
    "其中R(c|x)是条件风险.\n",
    "\n",
    "贝叶斯的目标是最小化结构风险，常用的损失函数是0-1损失函数。条件风险变成：$R(c|x) = 1 - p(c|x)$,最终目标函数变成：$h(x) = argmax P(c|x)$，其中P(c|x)是类别概率最大的概率。\n",
    "\n",
    "\n",
    "### 判别模型和生成模型\n",
    "\n",
    "判别模型：直接对P(c|x)进行建模  ->> 决策树，BP神经网络，SVM\n",
    "\n",
    "生成模型：学习联合概率P(c,x),然后的得到P(c|x)。 ->> 贝叶斯模型\n",
    "\n",
    "\n",
    "### 生成模型\n",
    "\n",
    "生成模型需要考虑：$P(c|x) = P(c, x)/P(x)$, 贝叶斯公式 $P(c|x) = P(x|c)*P(c)/P(x)$, 其中P(x)对全部的都一样，p(c)是类别的概率，先验概率，P(x|c)是条件概率。对属性x来说，如果有N个，则会变成：$p(x1, x2, ..., xn|C)$，如果属性之间有关系，则会生成N!个结果。\n",
    "\n",
    "\n",
    "### 极大似然估计\n",
    "\n",
    "对条件概率的学习一般会假定有一个概率分别形式，然后利用数据对参数进行估计。对于P(x|c)被唯一的参数$theta$确定，就是利用数据对P(x|theta)进行学习。概率模型的就是对参数进行学习。极大似然估计就是利用对数据进行抽样估计概率分布。\n",
    "\n",
    "似然函数$p(D_c|\\theta_c) = product(P(x|\\theta_c)$,其他$D_c$是c类样本的集合，目标对$\\theta_c$进行学习。连乘会下溢，使用对数似然：$LL(\\theta_c) = logP(D_c|\\theta_c) = \\sum(log(P(D_c|\\theta_c)))$\n",
    "\n",
    "\n",
    "### 朴素贝叶斯\n",
    "\n",
    "朴素贝叶斯假定就是全部的属性都是独立的，就不需要对不同的属性之间的概率进行计算，简化的计算。\n",
    "\n",
    "\n",
    "start to implement Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "data = np.array([[1, 2, 1, 2, 1], [1.2, 2.4, 6.2, 9.8, 2.2]])\n",
    "label = np.array([0, 1, 1, 0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.4, 1: 0.6}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# first to compute class probability\n",
    "counter = Counter(label)\n",
    "\n",
    "class_prob = {}\n",
    "\n",
    "for k, v in counter.items():\n",
    "    class_prob[k] = v/ len(label)\n",
    "class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is not for training, but pre-compute: {0: {1.0: 0.5, 2.0: 0.5}, 1: {2.0: 0.3333333333333333, 1.0: 0.6666666666666666}}\n",
      "continues type: {0: {'mean': 5.5, 'std': 4.3}, 1: {'mean': 3.6, 'std': 1.840289832245635}}\n"
     ]
    }
   ],
   "source": [
    "# next is to loop for each feature and based on each class to get prob.\n",
    "\n",
    "# based on feature type: category or contineous.\n",
    "\n",
    "# category\n",
    "def _get_cate(fea, label_counter):\n",
    "    cate_prob = {}\n",
    "    \n",
    "    for k in label_counter.keys():\n",
    "        k_data = fea[label == k]\n",
    "        \n",
    "        if k not in cate_prob:\n",
    "            cate_prob[k] = {}\n",
    "        \n",
    "        # get unique type of feature.\n",
    "        unique_k = Counter(k_data)\n",
    "        for uk, uv in unique_k.items():\n",
    "            prob_uk = uv / len(k_data)\n",
    "            cate_prob[k][uk] = prob_uk\n",
    "            \n",
    "    return cate_prob\n",
    "\n",
    "\n",
    "def _gaussian(data, mean, std):\n",
    "    return 1/(np.sqrt(2*np.pi)*std) * np.exp(- (data - mean)**2/2/std**2)\n",
    "\n",
    "\n",
    "def _get_con(fea, label_counter):\n",
    "    con_mean_std = {}\n",
    "    \n",
    "    for l in label_counter.keys():\n",
    "        k_data = fea[label == l]\n",
    "        \n",
    "        mean = k_data.mean()\n",
    "        std = k_data.std()\n",
    "        con_mean_std[l] ={}\n",
    "        \n",
    "        con_mean_std[l]['mean'] = mean\n",
    "        con_mean_std[l]['std']= std\n",
    "    \n",
    "    return con_mean_std\n",
    "        \n",
    "cate_prob = _get_cate(data[0, :], counter)\n",
    "con_mean_st = _get_con(data[1, :], counter)\n",
    "    \n",
    "print(\"this is not for training, but pre-compute:\", _get_cate(data[0, :], counter))\n",
    "print(\"continues type:\", _get_con(data[1, :], counter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5, 1: 0.6666666666666666}\n",
      "{0: 0.0666157863979957, 1: 0.14855286870976026}\n",
      "{0: 0.03330789319899785, 1: 0.09903524580650683}\n"
     ]
    }
   ],
   "source": [
    "# let's make prediction\n",
    "test_data = [1, 2.0]\n",
    "\n",
    "first_col_prob = {k:p.get(test_data[0]) for k, p in cate_prob.items()}\n",
    "print(first_col_prob)\n",
    "\n",
    "# continues compute\n",
    "second_col_prob ={}\n",
    "for k, mean_std in con_mean_st.items():\n",
    "    mean = mean_std['mean']\n",
    "    std = mean_std['std']\n",
    "    \n",
    "    second_col_prob[k] = _gaussian(test_data[1], mean, std)\n",
    "    \n",
    "print(second_col_prob)\n",
    "\n",
    "# let's combine it.\n",
    "out_prob = {}\n",
    "for k in counter.keys():\n",
    "    out_prob[k] = first_col_prob[k] * second_col_prob[k]\n",
    "    \n",
    "print(out_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get predict: 1\n"
     ]
    }
   ],
   "source": [
    "# only thing is to get lagest prob\n",
    "print(\"get predict:\", list(out_prob.keys())[np.argmax(list(out_prob.values()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
