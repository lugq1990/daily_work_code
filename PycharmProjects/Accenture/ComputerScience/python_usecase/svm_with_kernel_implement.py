# -*- coding:utf-8 -*-"""Created at 5:51 PM 12/3/2019This is to test with different kernel that we could implement toshow that with kernel, we could make un-linear data to be separated.Here is to show that with different feature function that we couldmake the un-separate class to be separated with hyperplane...@author: guangqiang.lu"""import numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom mpl_toolkits.mplot3d import Axes3Dfrom mpl_toolkits import mplot3dfrom sklearn.datasets.samples_generator import make_circlesfrom sklearn.svm import SVCx, y = make_circles(100, factor=.1, noise=.1)# --- feature list ---# 1. first feature map: (x1, x2, x1 **2 + x2**2)def feature_map_1(x):    return np.asarray((x[:, 0], x[:, 1], x[:, 0] ** 2 + x[:, 1] ** 2)).T# 2. RBF with center 0: (x1, x2, exp(-(x1**2 + x2**2)))def feature_map_2(x):    return np.asarray((x[:, 0], x[:, 1], np.exp(-(x[:, 0]**2 + x[:, 1]**2)))).T# 3. Polynomial function with degree with 2: (sqrt(2)* x1 * x2, x1**2, x2**2)def feature_map_3(x):    return np.asarray((np.sqrt(2)*x[:, 0] *x[:, 1] , x[:, 0]**2, x[:, 1] **2)).T# --- custom kernel(we could even create our new kernel function with sklearn) ---def my_kernel_1(x, y):    return np.dot(feature_map_1(x), feature_map_1(y).T)def my_kernel_2(x, y):    return np.dot(feature_map_2(x), feature_map_2(y).T)def my_kernel_3(x, y):    return np.dot(feature_map_3(x), feature_map_3(y).T)def my_kernel_test(kernel_list):    for kernel in kernel_list:        clf = SVC(kernel=kernel)        clf.fit(x, y)        print("for kernel %s with accuracy: %.2f" % (kernel.__name__, clf.score(x, y)))        if kernel_list.index(kernel) == len(kernel_list) - 1:            # initial step            h = .01            x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5            y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5            # meshgrid with location            xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))            # prediction on meshgrid            z = clf.predict(np.c_[xx.ravel(), yy.ravel()])            # reconvert to color plot            z = z.reshape(xx.shape)            # plot for now            plt.contourf(xx, yy, z, 1, colors=['darkblue', 'yellow'], alpha=.1)            plt.contour(xx, yy, z, cmap='cividis')            # plot training points            plt.scatter(x[:, 0], x[:, 1], c=y, cmap='cividis', edgecolors='k')            plt.title("with my kernel")            plt.show()def plot_figure(data, kernel_3=False):    fig = plt.figure(figsize=(16, 8))    ax = fig.add_subplot(1, 2, 1)    ax.scatter(x[:, 0], x[:, 1], c=y, cmap='viridis')    ax.set_xlabel('$x_1$')    ax.set_ylabel('$x_2$')    ax.set_title('original dataset')    ax = fig.add_subplot(1, 2, 2, projection='3d')    ax.scatter3D(data[:, 0], data[:, 1], data[:, 2], c=y, cmap='viridis')    ax.set_xlabel("$z_1$")    ax.set_ylabel('$z_2$')    ax.set_zlabel('$z_3$')    ax.set_title("new projection")    if kernel_3:        clf = SVC(kernel='linear')        clf.fit(data, y)        w = clf.coef_.flatten()        b = clf.intercept_.flatten()        # create x, y        xx, yy = np.meshgrid(np.linspace(-1, 1), np.linspace(0, 1))        # create corresponding z        boundary = (-w[0] * xx - w[1] * yy - b) * 1./w[2]        # plot the surface        ax.plot_surface(xx, yy, boundary, alpha=.3)    plt.show()# this is to test with custom featurefeature_data = [feature_map_1(x), feature_map_2(x), feature_map_3(x)]for i in range(len(feature_data)):    if i == 2:        plot_figure(feature_data[i], kernel_3=True)    else:        plot_figure(feature_data[i])# this is to test with sklearn my_kernelkernel_list = [my_kernel_1, my_kernel_2, my_kernel_3]my_kernel_test(kernel_list)