# -*- coding:utf-8 -*-"""Created at 5:18 PM 12/16/2019This is to test with gensim that we could continuous training our model with new documentadded, and we could save our trained model to disk and reloaded also with dictionary and corpus.@author: guangqiang.lu"""import osfrom gensim import corporafrom gensim import modelsfrom collections import defaultdictimport copy# this is try to solve Process finished with exit code -1073740940import localelocale.setlocale(locale.LC_ALL, 'de-CH')import tempfilepath = tempfile.mkdtemp()# path = 'C:/Users/guangqiiang.lu/Documents/lugq/github/hr_proj_data/whole_data'try:    path = os.path.join(path, 'gensim_test')    os.mkdir(path)    [os.remove(os.path.join(path, x)) for x in os.listdir(path)]except:    passdocuments = [    "Human machine interface for lab abc computer applications",    "A survey of user opinion of computer system response time",    "The EPS user interface management system",    "System and human system engineering testing of EPS",    "Relation of user perceived response time to error measurement",    "The generation of random binary unordered trees",    "The intersection graph of paths in trees",    "Graph minors IV Widths of trees and well quasi ordering",    "Graph minors A survey",]stoplist = set('for a of the and to in'.split())texts = [[word for word in text.lower().split()] for text in documents]# get frequency of wordsfre = defaultdict(int)for text in texts:    for word in text:        fre[word] += 1texts = [[token for token in text if fre[token] > 1] for text in texts]# build dictionary and corpusdct = corpora.Dictionary(texts)corpus = [dct.doc2bow(x) for x in texts]# start to train TFIDF modeltfidf = models.TfidfModel(corpus)# get tfidf converted corpuscorpus_tfidf = tfidf[corpus]# train our LSI modellsi_model = models.LsiModel(corpus_tfidf, id2word=dct, num_topics=2)print("Get LSI topics:", lsi_model.print_topics(2))# we have get whole things, now to save result to disk# 1. save dictionarydct.save(os.path.join(path, 'dictionary.dct'))# 2. save corpuscorpora.MmCorpus.serialize(os.path.join(path, 'corpus.mm'), corpus)# 3. save tfidf modeltfidf.save(os.path.join(path, 'tfidf.model'))# 4. save lsi modellsi_model.save(os.path.join(path, 'lsi.model'))# ----- then we could re-load our model ----dct_new = corpora.Dictionary.load(os.path.join(path, 'dictionary.dct'))corpus_new = corpora.MmCorpus(os.path.join(path, 'corpus.mm'))tfidf_new = models.TfidfModel.load(os.path.join(path, 'tfidf.model'))lsi_new = models.LsiModel.load(os.path.join(path, 'lsi.model'))print("Files are saved:", os.listdir(path))# ---- add a new document ----texts_new = "This is a tiny corpus of nine documents. each consisting of only a single sentence.".replace('.', '').split()dct_new.add_documents([texts_new])corpus_new = dct_new.doc2bow(texts_new)   # as this is just one new document# for TFIDF model, we could do online training, so we have to create this new model with whole new corpus# with new document added.corpus_new_whole = copy.deepcopy(corpus)corpus_new_whole.append(corpus_new)   # ensure with a list# recreate our TFIDF model with new whole datatfidf_new_whole = models.TfidfModel(corpus_new_whole)# after we have create new TFIDF model, then we could just get new document representation with new documentcorpus_tfidf_new_corpus = tfidf_new_whole[corpus_new]print("before we add document, we are ok.")print("TFIDF model training vector:", corpus_tfidf_new_corpus)# then we could do incremental learning with LSIlsi_new.add_documents([corpus_tfidf_new_corpus])print("New LSI model topics:", lsi_new.print_topics(2))print("new document representation with LSI:", lsi_new[corpus_new])print("Whole step finished without error!")