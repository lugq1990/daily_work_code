# -*- coding:utf-8 -*-"""Created at 4:19 PM 12/25/2019This is to test with SparkFile module that we could add a file to whole worker nodes,so that we could try to implement distributed model prediction.@author: guangqiang.lu"""import osfrom sklearn.linear_model import LogisticRegressionfrom sklearn.datasets import load_irisfrom pyspark.sql import SparkSessionfrom pyspark import SparkFilesfrom sklearn.externals import joblibpath = 'C:/Users/guangqiiang.lu/Documents/lugq/tensorflow_test'spark = SparkSession.builder.getOrCreate()sc = spark.sparkContextx, y = load_iris(return_X_y=True)lr = LogisticRegression()lr.fit(x, y)joblib.dump(lr, os.path.join(path, 'lr_iris.pkl'))# add trained model to spark nodesc.addFile(os.path.join(path, 'lr_iris.pkl'))# then we could re-loaded trained model from worker node temperate folderloaded_lr = joblib.load(SparkFiles.get('lr_iris.pkl'))# even with re-loaded model, we could do broadcast this model to whole workers, so that each worker# will have one instance that we could use for later step like prediction...sc.broadcast(loaded_lr)print("Get score from spark model:", loaded_lr.score(x, y))from keras.models import Sequentialmodel = Sequential()model.fit_generator()