# -*- coding:utf-8 -*-"""Created at 9:29 AM 1/7/2020Try to use Spark to do TFIDF and LDA model training sample to represent thatwe could use Spark to replace with gensim@author: guangqiang.lu"""import pandas as pdtexts = [    "Human machine interface for lab abc computer applications",    "A survey of user opinion of computer system response time",    "The EPS user interface management system",    "System and human system engineering testing of EPS",    "Relation of user perceived response time to error measurement",    "The generation of random binary unordered trees",    "The intersection graph of paths in trees",    "Graph minors IV Widths of trees and well quasi ordering",    "Graph minors A survey",]import osimport sysconfig = dict()config["spark_home"] = "/usr/hdp/current/spark2-client"config["pylib"] = "/python/lib"config['zip_list'] = ["/py4j-0.10.7-src.zip", "/pyspark.zip"]config['pyspark_python'] = "/anaconda-efs/sharedfiles/projects/mysched_9376/envs/cap_prd_py36_mml/bin/python"os.system("kinit -k -t /etc/security/keytabs/sa.mmlp.mysched.keytab sa.mmlp.mysched")os.environ["SPARK_HOME"] = config['spark_home']os.environ["PYSPARK_PYTHON"] = config["pyspark_python"]os.environ["PYLIB"] = os.environ["SPARK_HOME"] + config['pylib']zip_list = config['zip_list']for zip in zip_list:    sys.path.insert(0, os.environ["PYLIB"] + zip)# init spark sessionfrom pyspark.sql import SparkSessionfrom pyspark.ml.feature import IDF, Tokenizer, HashingTFfrom pyspark.mllib.clustering import LDAspark = SparkSession.builder.getOrCreate()df = pd.DataFrame(texts, columns=['sent'])df = spark.createDataFrame(df)tokenizer = Tokenizer(inputCol="sent", outputCol="words")words_df = tokenizer.transform(df)hashing_tf = HashingTF(inputCol='words', outputCol='raw_features')raw_df = hashing_tf.transform(words_df)idf = IDF(inputCol='raw_features', outputCol='features')idf_model = idf.fit(raw_df)idf_df = idf_model.transform(raw_df)# This should be changed.idf_rdd = idf_df.select(['features']).rdd# zip with each document with unique indexdata = idf_rdd.zipWithIndex().map(lambda x: x[x[1], x[0]])from pyspark.mllib.clustering import LDAlda_model = LDA.train(idf_rdd, k=3)# this test with pandas df and vectorimport numpy as npd = np.random.randn(10, 2)d = pd.DataFrame(d, columns=['a', 'b'])d = spark.createDataFrame(d)