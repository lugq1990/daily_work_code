# -*- coding:utf-8 -*-"""Created at 5:47 PM 11/1/2019This class is to implement the logger object that we coulduse for the project use case.@author: guangqiang.lu"""import loggingimport tempfileimport shutilimport osclass Logger(object):    def __init__(self, logger_name=None, logger_hdfs_path=None):        if logger_name is None:            logger_name = "project"   # we could change this.        self.logger_name = logger_name + '.log' if not logger_name.endswith('log') else logger_name        self._logger_path = tempfile.mkdtemp()        # self.logger_path = 'C:/Users/guangqiiang.lu/Documents/lugq/workings/pythonwhl'        self.logger = logging.getLogger(self.logger_name) if logger_name is not None else __file__        self.logger.setLevel(logging.INFO)        # this is to write logging info to disk        file_handler = logging.FileHandler(os.path.join(self._logger_path, self.logger_name))        formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s",                                      datefmt='%Y-%m-%d %H:%M:%S')        file_handler.setFormatter(formatter)        # this is to write the logging info to console        ch = logging.StreamHandler()        ch.setLevel(logging.INFO)        ch.setFormatter(formatter)        self.logger.addHandler(ch)        self.logger.addHandler(file_handler)        if logger_hdfs_path is None:            raise ValueError("We are going to put logger file to HDFS, you have to give HDFS path that logger stores!")        self.logger_hdfs_path = logger_hdfs_path    def info(self, msg):        self.logger.info(msg)    def warning(self, msg):        self.logger.warning(msg)    def error(self, msg):        self.logger.error(msg)    def debug(self, msg):        self.logger.debug(msg)    def exception(self, msg):        self.logger.exception(msg)    def critical(self, msg):        self.logger.critical(msg)    def upload_logger_file(self, sc):        """This is to write the local file to HDFS When there is any error during the program."""        try:            with open(os.path.join(self._logger_path, self.logger_name), 'r') as f:                logger_str = '\n'.join(f.readlines())        except Exception as e:            raise IOError("When we want to read the file, we couldn't find logger file in local path.")        logger_string_rdd = sc.parallelize([logger_str])        # ensure we get the hdfs link        hdfs_file_path = "hdfs://" + os.path.join(self.logger_hdfs_path, self.logger_name) if not \            self.logger_hdfs_path.startswith('hdfs://') \            else os.path.join(self.logger_hdfs_path, self.logger_name)        logger_string_rdd.saveAsTextFile(hdfs_file_path)        # if whole things finish, we should remove the temperate folder        try:            shutil.rmtree(self._logger_path)        except Exception as e:            raise IOError("When to remove the temprate folder with error: %s" % e)    def upload_files_with_sys(self):        """this is to use system command to upload the file to hdfs,        as with spark, I have to read the local file as a DataFrame        that I could use `overwrite` mode, with RDD's saveAsTextFile,        will check with file exists or not(Or use RDD to set the spark        context:set("spark.hadoop.validateOutputSpecs", "false") avoid        error)"""        upload_command = "hdfs dfs -put -f %s %s" % (os.path.join(self._logger_path, self.logger_name),                                                     os.path.join(self.logger_hdfs_path, self.logger_name))        self.logger.info("Now to run command: %s" % upload_command)        try:            os.system(upload_command)        except Exception as e:            logger.error("When to upload file to HDFS with error %s" % e)            raise OSError("When to upload file to HDFS with error %s" % e)        # we should remove the temperate folder that we created on local disk        try:            shutil.rmtree(self._logger_path)        except Exception as e:            # here change not to raise error, but we should warn the user that the folder we created            # haven't been deleted, as even if the folder haven't been created doesn't affect the main logic.            # raise IOError("When to remove the temperate folder with error %s. " % e)            self.warning("When to remove the temperate folder with error %s. " % e)        self.logger.info("Logger file already upload to HDFS!")def create_sc():    import sys    config = dict()    config["spark_home"] = "/usr/hdp/current/spark2-client"    config["pylib"] = "/python/lib"    config['zip_list'] = ["/py4j-0.10.7-src.zip", "/pyspark.zip"]    config['pyspark_python'] = "/anaconda-efs/sharedfiles/projects/mysched_9376/envs/cap_prd_py36_mml/bin/python"    os.system("kinit -k -t /etc/security/keytabs/sa.mmld.mysched.keytab sa.mmld.mysched")    os.environ["SPARK_HOME"] = config['spark_home']    os.environ["PYSPARK_PYTHON"] = config["pyspark_python"]    os.environ["PYLIB"] = os.environ["SPARK_HOME"] + config['pylib']    zip_list = config['zip_list']    for zip in zip_list:        sys.path.insert(0, os.environ["PYLIB"] + zip)    # This module must be imported after environment init.    from pyspark.sql import SparkSession    from pyspark import SparkConf    conf = SparkConf().setAppName("create_mapping").setMaster("yarn")    spark = SparkSession.builder.config(conf=conf).config('spark.driver.memory', '3g').enableHiveSupport().getOrCreate()    return spark.sparkContextif __name__ == '__main__':    hdfs_path = '/data/discovery/mysched/log_test'    logger = Logger(logger_hdfs_path=hdfs_path)    logger.info("This is info logic.")    logger.debug("Now to debug.")    logger.warning("There is something not rigth.")    # logger.exception("There is a exception.")    logger.error("There is Error! Should exit!")    # When there is error happens, then we should put the logging file to HDFS    # logger.upload_logger_file(create_sc())    # this to use command to upload file    logger.upload_files_with_sys()    logger.info("Whole things finished without error.")