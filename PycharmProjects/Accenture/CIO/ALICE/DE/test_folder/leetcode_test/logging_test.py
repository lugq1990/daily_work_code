# -*- coding:utf-8 -*-"""This is to test for python logging logic to write logging file,"""import loggingimport osimport tracebackpath = 'C:/Users/guangqiiang.lu/Documents/lugq/workings/pythonwhl'# hdfs_path = 'hdfs://name-node.cioprd.local:8020/data/insight/cio/alice.pp/logging_test'# in fact if we set the config with basic config, then the whole process logging# with be same. so we could set with different logger object.# logging.basicConfig(filename=os.path.join(path, 'logging_test.txt'),#                     filemode='a',#                     format='%(asctime)s::%(name)s %(levelname)s :: %(funcName)s: %(message)s',#                     datefmt='%Y:%m:%d %H:%M:%S',#                     level=logging.DEBUG)logger = logging.getLogger("logging_test_1")logger.setLevel(logging.INFO)file_handler = logging.FileHandler(os.path.join(path, 'logging_1.log'))formatter = logging.Formatter('%(asctime)s:: %(name)s::%(levelname)s::%(message)s')file_handler.setFormatter(formatter)logger.addHandler(file_handler)logger2 = logging.getLogger("logging_test_2")logger2.setLevel(logging.INFO)file_handler2 = logging.FileHandler(os.path.join(path, 'logging_2.log'))file_handler2.setFormatter(formatter)logger2.addHandler(file_handler2)logging.info('start to test.')logging.debug('debug the code without the function, this should be in the root user.')# logger = logging.getLogger(__name__)logger.info('go go go.')logger2.info("this is for logging 2 function.")logger2.warning('something is not good with warning in logger2.')def test():    try:        1 / 0    except Exception as e:        logger.error("when do transfer with error %s" % traceback.format_exc())        raise elogger.error('This should been stored into the logging file.')# """This is to get the whole files in the MMR s3 bucket."""# import boto3# import pandas as pd# import os## local_path = '/mrsprd_data/Users/ngap.app.alice/shell_test/mapping_shell/other_python'## secret_key = 'Z4d6tlLDRipLWmR43calDvE2SgxHagHNQa8ZBaV9'# access_key = 'AKIAJMR43CLDBRZMVLAA'# bucket_name = '4339-mmr-prod'## s3 = boto3.Session(aws_access_key_id=access_key, aws_secret_access_key=secret_key).resource('s3')# my_bucket = s3.Bucket(bucket_name)## file_list = []# prefix_list = ['DocStore/', 'DocStore2/', 'DocStore3/']# for p in prefix_list:#     curr_list = []#     for f in my_bucket.objects.filter(Prefix=p):#         if '.' not in f.key:#             continue#         curr_list.append(f.key)#         if len(curr_list) % 10000 == 0:#             print('Already get %d file from %s' % (len(curr_list), p))#     file_list.extend(curr_list)## file_df = pd.DataFrame(file_list, columns=['file_name'])## file_df.to_csv(os.path.join(local_path, 'whole_file_in_mmr.csv'), header=False, index=False)##### import boto3# import pandas as pd# import os# import paramiko## sftp_path = '/sftp/cio.alice'## # init the paramiko# host = '10.5.105.51'# port = 22# username = 'ngap.app.alice'# password = 'QWer@#2019'## ssh = paramiko.SSHClient()# ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())## ssh.connect(host, port=port, username=username, password=password, look_for_keys=False)## access_key = 'AKIAJGK5GH7CU46OSS5Q'# secret_key = 'e8PqI7LCJNlckET3i6SeHLPWY4/gJkec28elTfMF'# bucket_name = 'aliceportal-30899-prod'# folder_name = 'Delta_Missing_PDFwithDOCX'## session = boto3.Session(aws_access_key_id=access_key, aws_secret_access_key=secret_key)# client = session.client('s3')# s3 = session.resource('s3')# my_bucket = s3.Bucket(bucket_name)## local_path = '/mrsprd_data/Users/ngap.app.alice'# file_name = '359_unsupported_type.csv'# # read files with pandas. without extension.# df = pd.read_csv(os.path.join(local_path, file_name), names=['file_name', 'file_path'])## file_list = df['file_name'].values.tolist()# file_list = [x + '.txt' for x in file_list]# # loop for whole folder to get the file needed to be download# already_download = []# for f in my_bucket.objects.filter(Prefix=folder_name + '/'):#     name = f.key.split('/')[-1]#     if name in file_list:#         already_download.append(name)#         file = client.get_object(Bucket=bucket_name, Key=f.key)['Body'].read()  # write it to memory#         try:#             sftp = ssh.open_sftp()#             with sftp.open(os.path.join(sftp_path, name), 'w') as f_w:#                 f_w.write(file)#         except Exception as e:#             continue#         if len(already_download) % 100 == 0:#             print('already download %d files.' % len(already_download))## s3_list = []# for f in my_bucket.objects.filter(Prefix=folder_name + '/'):#     if f.key.endswith('.txt'):#         s3_list.append(f.key)#     if len(s3_list) % 100000 == 0:#         print("already get %d files" % len(s3_list))## s3_list = [x.split('/')[-1] for x in s3_list]# # get whether the file already in s3# common_list = list(set(s3_list).intersection(set(file_list)))# print("there are %d common files." % len(common_list))### new_df = pd.read_csv(os.path.join(local_path, 's3_catchup_docx.csv'), names=['file_name'])# new_list = new_df['file_name'].values.tolist()# new_list = [x + '.txt' for x in new_list]## file_list += new_list## hdfs_file_name = '/data/insight/cio/alice/hivetable/documents_name/whole_file.csv'# from hdfs.ext.kerberos import KerberosClient## client = KerberosClient("http://name-node.cioprd.local:50070;http://name-node2.cioprd.local:50070")## client.download(hdfs_path=hdfs_file_name, local_path=os.path.join(local_path, hdfs_file_name.split('/')[-1]), overwrite=True)## hdfs_df = pd.read_csv(os.path.join(local_path, hdfs_file_name.split('/')[-1]), names=['file_name', 'a', 'path', 'c', 'd'])## common_df = pd.merge(hdfs_df, df, on='file_name', how='inner')## file_name_list = hdfs_df['file_name'].values.tolist()# for i in range(len(df)):#     if df.iloc[i, 0] in file_name_list:#         hdfs_df.loc[hdfs_df['file_name'] == df['file_name'].values.tolist()[i], 'path'] = df['file_path'].values.tolist()[i]#         if i % 100 == 0:#             print("already changed %d files" % i)## new_df = pd.merge(df, hdfs_df, on='file_name', how='inner')## check_df = pd.merge(df, new_df, on=['file_name', 'file_path'], how='inner')## hdfs_df.to_csv(os.path.join(local_path, hdfs_file_name.split('/')[-1]), header=False, index=False)## client.upload(hdfs_path=hdfs_file_name, local_path=os.path.join(local_path, hdfs_file_name.split('/')[-1]), overwrite=True)